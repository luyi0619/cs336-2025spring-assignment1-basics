{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52aabf6-7254-4b69-bdfd-e5802d94376f",
   "metadata": {},
   "source": [
    "What Unicode character does chr(0) return? The chr(0) function returns the NULL character (U+0000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697f7ccb-7a83-470f-925b-02053a9ab47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57bade-a34d-4803-b527-35257474aada",
   "metadata": {},
   "source": [
    "How does this character’s string representation (__repr__()) differ from its printed representa-\n",
    "tion? The string representation (__repr__()) of the NULL character shows it as '\\x00', while its printed representation shows nothing at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cbe8ee-5c36-4a4b-bc8a-943957093046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0).__repr__() # There are 6 characters in chr(0).__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3303f-1f0c-41d2-9a1c-4d03736970d1",
   "metadata": {},
   "source": [
    "What happens when this character occurs in text? When the NULL character occurs in text, it often acts as a terminator or a placeholder and is typically not rendered visually, meaning it will appear as an invisible character or truncate the string in some environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b71c33-b709-4188-9909-05b77c6d0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\"\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0118625d-3d37-4458-a618-a2e788da0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "13\n",
      "23\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "list(utf8_encoded)\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33840afd-4fcb-4456-b60b-de517edca9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "13\n",
      "28\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(utf16_encoded)\n",
    "print(type(utf16_encoded))\n",
    "list(utf16_encoded)\n",
    "print(len(test_string))\n",
    "print(len(utf16_encoded))\n",
    "print(utf16_encoded.decode(\"utf-16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9c8a6d-724f-440b-9fa1-ec7928cace55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "13\n",
      "56\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "list(utf32_encoded)\n",
    "print(len(test_string))\n",
    "print(len(utf32_encoded))\n",
    "print(utf32_encoded.decode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0e655-9df4-4c14-b5db-5d3edbccfb59",
   "metadata": {},
   "source": [
    "What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings. \n",
    "\n",
    "What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07472139-6880-4b78-ba89-95530fcb958a",
   "metadata": {},
   "source": [
    "Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1150af8-5a00-4a1d-a6bb-09671a429989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "  return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1563bb-4f77-44ea-8e3e-62332963e877",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3311871793.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mAn example input byte string for which decode_utf8_bytes_to_str_wrong pro-\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "An example input byte string for which decode_utf8_bytes_to_str_wrong pro-\n",
    "duces incorrect output, with a one-sentence explanation of why the function is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19ad5-3fb0-4ebc-ba46-c465a6a80acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"は\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad94a3c-1d54-4728-b31c-8c9a58476464",
   "metadata": {},
   "source": [
    "Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa1e2b-5c1b-487b-b897-b9cc4b6135b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([0, 128]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0be422-ebfb-4b4c-af75-d983ed55b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce5597e-4d96-420e-8dc2-3cb04781eab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b4956-5ea2-4d74-b6a6-61b1614dc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "  pairs = collections.defaultdict(int)\n",
    "  for word, freq in vocab.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "      pairs[symbols[i],symbols[i+1]] += freq\n",
    "  return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "  v_out = {}\n",
    "  bigram = re.escape(' '.join(pair))\n",
    "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "  for word in v_in:\n",
    "    w_out = p.sub(''.join(pair), word)\n",
    "    v_out[w_out] = v_in[word]\n",
    "  return v_out\n",
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
    "'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "  pairs = get_stats(vocab)\n",
    "  best = max(pairs, key=pairs.get)\n",
    "  vocab = merge_vocab(best, vocab)\n",
    "  print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e133afd-1185-4189-a7c4-fd867a709a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tokens: dict[tuple[bytes], int] = { ('l', 'o', 'w') :5, ('l', 'o', 'w', 'e', 'r') :2, ('w', 'i', 'd' , 'e' , 's' , 't'): 3, ('n', 'e', 'w' , 'e' , 's' , 't'): 6  }\n",
    "vocab = ['<|endoftext|>'] + [chr(b) for b in range(256)]\n",
    "\n",
    "def merge(tokens: dict[tuple[bytes], int], vocab: list[tuple[bytes]], merges: list[tuple[bytes, bytes]]):\n",
    "  pair_dict: dict[tuple[tuple[bytes]], int] = defaultdict(int)\n",
    "  for token, cnt in tokens.items():\n",
    "    for i in range(1, len(token)):\n",
    "      new_pair = (token[i-1], token[i])\n",
    "      pair_dict[new_pair] = pair_dict[new_pair] + cnt\n",
    "\n",
    "  max_cnt = 0\n",
    "  max_pair = None\n",
    "  for pair, cnt in pair_dict.items():\n",
    "    if cnt > max_cnt or (cnt == max_cnt and pair > max_pair):\n",
    "      max_cnt = cnt\n",
    "      max_pair = pair\n",
    "\n",
    "  print(\"!!!\", max_pair)\n",
    "  max_pair_concat = max_pair[0] + max_pair[1]  \n",
    "  merges.append((max_pair[0], max_pair[1]))\n",
    "  vocab[len(vocab)] = max_pair\n",
    "  print(\"@@@\", max_pair_concat)\n",
    "  print(\"###\", merges[-1])\n",
    "\n",
    "  new_tokens: dict[tuple[tuple[bytes]], int]  = {}\n",
    "  for token, cnt in tokens.items():\n",
    "    new_token = []\n",
    "    for i in range(len(token)):\n",
    "        new_token.append(token[i])\n",
    "        if len(new_token) >= 2 and (new_token[-2], new_token[-1]) == max_pair:\n",
    "            new_token[-2:] = (max_pair_concat,)\n",
    "    new_tokens[tuple(new_token)] = cnt\n",
    "\n",
    "  return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8259e736-3299-4538-a298-da909292e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! (b'v', b'e')\n",
      "@@@ b've'\n",
      "### (b'v', b'e')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(b'u',): 2,\n",
       " (b' ', b'd', b'o', b'n'): 1,\n",
       " (b\"'\", b't'): 1,\n",
       " (b' ', b'h', b'a', b've'): 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_token_count_pair(input: list[str]):\n",
    "    token = defaultdict(int)\n",
    "    for i in input:\n",
    "        x = tuple([bytes(tuple([b])) for b in i])\n",
    "        token[x] += 1\n",
    "    return token\n",
    "\n",
    "input = [ t.encode(\"utf-8\") for t in ['u', ' don', \"'t\", ' have', 'u']]\n",
    "merges = []\n",
    "tokens = get_token_count_pair(input)\n",
    "vocab = {}\n",
    "tokens = merge(tokens, vocab, merges)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4452865-a61e-4507-8f53-8e4a43e4fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Doc 1]', '[Doc 2]', '[Doc 1]', '[Doc 2]']\n"
     ]
    }
   ],
   "source": [
    "def split_by_sepcial_tokens(text: str, special_tokens: list[str]):\n",
    "  escape_special_tokens = [re.escape(s) for s in special_tokens]\n",
    "  return re.split(\"|\".join(escape_special_tokens), text)\n",
    "\n",
    "text = \"[Doc 1]<|endoftext|>[Doc 2]<|endofline|>[Doc 1]<|endoftext|>[Doc 2]\"\n",
    "special_tokens = [\"<|endoftext|>\", \"<|endofline|>\"]\n",
    "print(split_by_sepcial_tokens(text, special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d7d2400-7faa-4675-9344-63d507d70781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_and_encode(docs: list[str]):\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        splits = re.findall(PAT, doc)\n",
    "        result += [s.encode(\"utf-8\") for s in splits]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0d8ae94-c950-4411-87e1-a4ea8dd6a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretokenization_example import find_chunk_boundaries\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]):\n",
    "  vocab: dict[int, bytes] = {}\n",
    "  merges: list[tuple[bytes, bytes]] = []\n",
    "\n",
    "  num_processes = 10\n",
    "  with open(input_path, \"rb\") as f:\n",
    "    boundaries = find_chunk_boundaries(\n",
    "        f, num_processes, \"<|endoftext|>\".encode(\"utf-8\")\n",
    "    )\n",
    "    f.seek(0)\n",
    "    # read the first segments for now\n",
    "    text = f.read(boundaries[1]).decode(\"utf-8\")\n",
    "    text_split_by_sepcial_tokens = split_by_sepcial_tokens(text, special_tokens)\n",
    "    encoded_tokens = pre_tokenize_and_encode(text_split_by_sepcial_tokens)\n",
    "\n",
    "    vocab[0] = '<|endoftext|>'\n",
    "    for i in range(256):\n",
    "        vocab[i+1] = chr(i)\n",
    "    token_cnt_pairs = get_token_count_pair(encoded_tokens)\n",
    "\n",
    "    num_steps = vocab_size - len(vocab)\n",
    "    for i in range(num_steps):\n",
    "      token_cnt_pairs = merge(token_cnt_pairs, vocab, merges)\n",
    "  return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc16dc2a-fa0b-4416-bb2b-429aec0c41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), (b' t', b'he'), (b'e', b'd'), (b' ', b'b'), (b' t', b'o'), (b' a', b'nd'), (b' ', b'h'), (b' ', b'f'), (b'i', b'n'), (b' w', b'a'), (b' ', b'T'), (b'r', b'e'), (b'i', b't'), (b'o', b'u'), (b' ', b'l'), (b' ', b'd'), (b' ', b'c'), (b' ', b'p'), (b'a', b'y'), (b' wa', b's'), (b'e', b'r'), (b' ', b'm'), (b'o', b'm'), (b' ', b'he'), (b' T', b'he'), (b'i', b's'), (b' ', b'n'), (b'o', b'n'), (b' s', b'a'), (b'i', b'd'), (b'l', b'l'), (b'a', b'r'), (b'i', b'm'), (b' h', b'a'), (b'a', b't'), (b' ', b'g'), (b' ', b'S'), (b'o', b't'), (b'in', b'g'), (b'e', b'n'), (b'a', b'n'), (b'l', b'e'), (b'o', b'r'), (b'i', b'r'), (b'a', b'm'), (b' ', b'H'), (b'e', b't'), (b' t', b'h'), (b' ', b'it'), (b'i', b'g'), (b'i', b'l'), (b' The', b'y'), (b' H', b'e'), (b' ', b'in'), (b' ', b'\"'), (b' p', b'l'), (b'v', b'er'), (b'o', b'w'), (b'r', b'i'), (b'u', b't'), (b' ', b'u'), (b' sa', b'id'), (b' d', b'ay'), (b'p', b'p'), (b'it', b'h'), (b' ', b'o'), (b' w', b'ith'), (b' ', b'y'), (b' b', b'e'), (b'O', b'n'), (b' pl', b'ay'), (b'o', b'o'), (b' ', b'r'), (b' he', b'r'), (b'k', b'ed'), (b' ', b'I'), (b'c', b'e'), (b' S', b'he'), (b' h', b'is'), (b'l', b'd'), (b' s', b't'), (b'k', b'e'), (b' ', b'e'), (b' T', b'im'), (b'n', b't'), (b' y', b'ou'), (b'v', b'e'), (b'c', b'k'), (b' b', b'ig'), (b'ver', b'y'), (b's', b't'), (b' ', b'on'), (b' ha', b'pp'), (b'il', b'y'), (b' ', b'L'), (b'e', b'nd'), (b'u', b'n'), (b'a', b'll'), (b'ri', b'end'), (b' f', b'riend'), (b' u', b'p'), (b' n', b'ot'), (b' w', b'e'), (b' the', b'y'), (b' wa', b'nt'), (b' ha', b'd'), (b' d', b'o'), (b'it', b't'), (b' l', b'i'), (b' o', b'f'), (b'he', b'r'), (b' ', b'B'), (b'e', b's'), (b'a', b'd'), (b' happ', b'y'), (b' ', b'M'), (b' sa', b'w'), (b's', b'e'), (b' th', b'at'), (b'On', b'e'), (b'en', b't'), (b' ', b'very'), (b' m', b'om'), (b'itt', b'le'), (b' l', b'ittle'), (b'ou', b'ld'), (b' s', b'he'), (b' f', b'or'), (b\"'\", b's'), (b' s', b'o'), (b' s', b'h'), (b'im', b'e'), (b' ', b'k'), (b' n', b'e'), (b' b', b'o'), (b' t', b'ime'), (b'.', b'\"'), (b' n', b'am'), (b' L', b'ily'), (b'ou', b'nd'), (b'c', b'h'), (b' the', b're'), (b' nam', b'ed'), (b' s', b'm'), (b' b', b'ir'), (b' want', b'ed'), (b' we', b're'), (b' b', b'ut'), (b'!', b'\"'), (b' bir', b'd'), (b' friend', b's'), (b'T', b'he'), (b'v', b'ed'), (b' T', b'om'), (b'ou', b't'), (b'h', b't'), (b'a', b'l'), (b' a', b'n'), (b' I', b't'), (b'id', b'e'), (b'l', b'p'), (b'a', b'ke'), (b'e', b'l'), (b' to', b'o'), (b' he', b'lp'), (b'On', b'ce'), (b' w', b'h'), (b' a', b'll'), (b' ', b'A'), (b'om', b'e'), (b' w', b'ent'), (b' ', b'is'), (b' l', b'o'), (b' up', b'on'), (b' l', b'oo'), (b'u', b'g'), (b'u', b'e'), (b'r', b'y'), (b't', b'er'), (b' to', b'y'), (b'o', b're'), (b'i', b'nd'), (b'g', b'et'), (b' f', b'un'), (b'i', b'll'), (b' a', b's'), (b' c', b'at'), (b'get', b'her'), (b'u', b'r'), (b' d', b'id'), (b'am', b'e'), (b' a', b't'), (b' ', b'j'), (b' to', b'gether'), (b' ', b're'), (b' b', b'a'), (b' do', b'g'), (b'e', b'c'), (b' t', b're'), (b'i', b'c'), (b' g', b'ir'), (b'oo', b'd'), (b' gir', b'l'), (b'm', b'y'), (b'l', b'y'), (b' r', b'o'), (b' c', b'an'), (b'a', b'in'), (b' k', b'n'), (b't', b'ed'), (b' c', b'ould'), (b' the', b'ir'), (b' b', b'all'), (b'ar', b'd'), (b' s', b'e'), (b'w', b'ay'), (b' h', b'im'), (b' l', b'e'), (b'he', b'd'), (b'ar', b'k'), (b' the', b'm'), (b'r', b'a'), (b' play', b'ed'), (b'?', b'\"'), (b'u', b'm'), (b'a', b'x'), (b' ', b'out'), (b' a', b're'), (b' f', b'r'), (b'he', b'n'), (b' sa', b'd'), (b' g', b'o'), (b' bo', b'y'), (b' ha', b've'), (b' c', b'l'), (b\"'\", b't'), (b'ou', b'g'), (b'u', b'l')]\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = train_bpe('../data/TinyStoriesV2-GPT4-valid.txt', 500, [\"<|endoftext|>\"])\n",
    "print (merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca2c3a-6dfc-4281-a4e8-cbd0cf0fdd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
